{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [project page](https://wandb.ai/mines-paristech-cmm/fcdd-mvtec-dev00-checkpoint02)\n",
    "\n",
    "i will add the test avg precision metric to some old runs and compute a stability metric for auc/avg-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a cell print all the outputs instead of just the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_ENTITY = \"mines-paristech-cmm\"\n",
    "WANDB_PROJECT = \"fcdd-mvtec-dev00-checkpoint02\"\n",
    "WANDB_ENTITY_PROJECT = f\"{WANDB_ENTITY}/{WANDB_PROJECT}\"\n",
    "\n",
    "WANDB_SWEEP_ID = None\n",
    "WANDB_SWEEP_PATH = f\"{WANDB_ENTITY_PROJECT}/{WANDB_SWEEP_ID}\" if WANDB_SWEEP_ID else None\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()\n",
    "\n",
    "sweep = api.sweep(WANDB_SWEEP_PATH) if WANDB_SWEEP_PATH else None\n",
    "runs = api.runs(WANDB_ENTITY_PROJECT) if sweep is None else sweep.runs\n",
    "\n",
    "from collections import defaultdict\n",
    "lists = defaultdict(list)\n",
    "\n",
    "def append_to_list(key, value):\n",
    "    lists[key].append(value)\n",
    "\n",
    "for run in runs: \n",
    "    append_to_list(\"summary\", run.summary._json_dict)\n",
    "    append_to_list(\"config\", {k: v for k,v in run.config.items() if not k.startswith('_')})\n",
    "    append_to_list(\"name\", run.name)\n",
    "    append_to_list(\"tags\", run.tags)\n",
    "    append_to_list(\"id\", run.id)\n",
    "    append_to_list(\"state\", run.state)\n",
    "    \n",
    "import pandas as pd\n",
    "runs_df = pd.DataFrame.from_dict(data=lists)\n",
    "\n",
    "runs_df.shape\n",
    "runs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_keys = sorted(set.union(*runs_df[\"summary\"].apply(lambda x: set(x.keys())).tolist()))\n",
    "\"summary\"\n",
    "\"   \".join(summary_keys)\n",
    "\n",
    "config_keys = sorted(set.union(*runs_df[\"config\"].apply(lambda x: set(x.keys())).tolist()))\n",
    "\"config\"\n",
    "\"   \".join(config_keys)\n",
    "\n",
    "tags = sorted(set.union(*runs_df[\"tags\"].apply(lambda x: set(x)).tolist()))\n",
    "\"tags\"\n",
    "\"   \".join(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_df_filtered = runs_df\n",
    "\n",
    "is_report_bmvc_02 = runs_df['tags'].apply(lambda x: \"report-bmvc-02\" in x)\n",
    "runs_df_filtered = runs_df[is_report_bmvc_02]\n",
    "\n",
    "runs_df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract(df, from_column, key):\n",
    "    return df[from_column].apply(lambda x: x.get(key, None))\n",
    "\n",
    "df = runs_df_filtered\n",
    "config_keys = [\n",
    "    \"loss_mode\",\n",
    "    \"noise_mode\",\n",
    "    \"normal_class\",\n",
    "    \"normal_class_label\",\n",
    "    \"logdir\",\n",
    "    \"datadir\",\n",
    "    \"preproc\",\n",
    "    \"batch_size\",\n",
    "]\n",
    "summary_keys = [\n",
    "    \"test_rocauc\",\n",
    "]\n",
    "for key in config_keys:\n",
    "    df[key] = extract(df, \"config\", key)\n",
    "for key in summary_keys:\n",
    "    df[key] = extract(df, \"summary\", key)\n",
    "del df\n",
    "runs_df_filtered.columns\n",
    "runs_df_filtered[config_keys].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorical vals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in [\n",
    "    \"loss_mode\",\n",
    "    \"noise_mode\",\n",
    "    \"normal_class_label\",\n",
    "]:\n",
    "    print(f\"{key}: {runs_df_filtered[key].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get metric histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "df = runs_df_filtered\n",
    "keys = [\n",
    "]\n",
    "\n",
    "# get history of signals\n",
    "hists_per_run = []\n",
    "\n",
    "for run_id in progressbar.progressbar(df[\"id\"], max_value=df.shape[0]):\n",
    "    run = api.run(f\"{WANDB_ENTITY_PROJECT}/{run_id}\")\n",
    "    hist = run.scan_history(keys=keys)\n",
    "    hists_per_run.append([np.array([dic[key] for dic in hist]) for key in keys])\n",
    "\n",
    "# transpose the list of lists\n",
    "hists_per_run = list(map(list, zip(*hists_per_run)))\n",
    "\n",
    "\n",
    "for key, list_ in zip(keys, hists_per_run):\n",
    "    df[key] = list_\n",
    "    \n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find model params and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arow = runs_df_filtered.iloc[0]\n",
    "arow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "dev_dir = Path(\"../python/dev\").resolve()\n",
    "dev_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../python/dev/mvtec_dataset_dev01_bis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../python/dev/common_dev01_bis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../python/dev/data_dev01_bis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../python/dev/model_dev01_bis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../python/dev/callbacks_dev01_bis.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ../python/dev/hacked_dev01.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh *_dev01_bis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(ds, batch_size=16, num_workers=0)\n",
    "from mvtec_dataset_dev01_bis import MVTecAnomalyDetectionDataModule, DATAMODULE_PREPROCESS_MOMENT_BEFORE_BATCH_TRANSFER, SUPERVISE_MODE_REAL_ANOMALY\n",
    "datamodule = MVTecAnomalyDetectionDataModule(\n",
    "    root=(dev_dir / arow[\"datadir\"]).resolve(),\n",
    "    normal_class=arow[\"normal_class\"],\n",
    "    preprocessing=arow[\"preproc\"],\n",
    "    preprocess_moment=DATAMODULE_PREPROCESS_MOMENT_BEFORE_BATCH_TRANSFER,\n",
    "    supervise_mode=SUPERVISE_MODE_REAL_ANOMALY,\n",
    "    batch_size=int(arow[\"batch_size\"]),\n",
    "    nworkers=0,\n",
    "    pin_memory=False,\n",
    "    seed=0,\n",
    "    raw_shape=(240, 240),\n",
    "    net_shape=(224, 224),\n",
    "    real_anomaly_limit=1,\n",
    ")\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from fcdd.models.fcdd_cnn_224 import FCDD_CNN224_VGG_F\n",
    "from model_dev01_bis import FCDD\n",
    "\n",
    "logdir = arow[\"logdir\"]\n",
    "\n",
    "net = FCDD(\n",
    "    in_shape=(224, 224),\n",
    "    model_name=\"FCDD_CNN224_VGG_F\",\n",
    "    # these values dont matter\n",
    "    optimizer_name=\"sgd\",  \n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    scheduler_name=\"lambda\",\n",
    "    scheduler_parameters=[.999],\n",
    "    loss_name=\"old-fcdd\",\n",
    "    dropout_mode=None,\n",
    "    dropout_parameters=[],\n",
    ")\n",
    "snapshot_fpath = (dev_dir / logdir / \"snapshot.pt\").resolve()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    snapshot = torch.load(snapshot_fpath)\n",
    "\n",
    "else:\n",
    "    snapshot = torch.load(snapshot_fpath, map_location=torch.device('cpu'))\n",
    "\n",
    "net_state = snapshot.pop('net', None)\n",
    "assert net_state is not None\n",
    "net.load_state_dict(net_state)\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_outputs = []\n",
    "# for idx, batch in enumerate(datamodule.test_dataloader(embed_preprocessing=True)):\n",
    "#     steps_outputs.append(net.test_step(batch, idx));\n",
    "# net.test_epoch_end(steps_outputs);\n",
    "# net.last_epoch_outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "wandb_logger = WandbLogger(\n",
    "    id=arow[\"id\"],\n",
    "    project=WANDB_PROJECT,\n",
    "    entity=WANDB_ENTITY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from callbacks_dev01_bis import LogPrcurveCallback\n",
    "from common_dev01_bis import create_python_random_generator\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    gpus=1, \n",
    "    logger=wandb_logger,  \n",
    "    callbacks=[\n",
    "        LogPrcurveCallback(\n",
    "            scores_key=\"score_maps\",\n",
    "            gt_key=\"gtmaps\",\n",
    "            log_curve=False,\n",
    "            limit_points=None,\n",
    "            # doesnt matter because limit_points is None\n",
    "            python_generator=create_python_random_generator(0),  \n",
    "            stage=\"test\",\n",
    "        )\n",
    "    ], \n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=net, datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.close()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test all runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvtec_dataset_dev01_bis import MVTecAnomalyDetectionDataModule, DATAMODULE_PREPROCESS_MOMENT_BEFORE_BATCH_TRANSFER, SUPERVISE_MODE_REAL_ANOMALY\n",
    "import torch\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from model_dev01_bis import FCDD\n",
    "from pytorch_lightning import Trainer\n",
    "from callbacks_dev01_bis import LogPrcurveCallback\n",
    "from common_dev01_bis import create_python_random_generator\n",
    "\n",
    "for idx, (rowidx, row) in enumerate(runs_df_filtered.iterrows()):\n",
    "    \n",
    "    if idx >= 6:\n",
    "        break\n",
    "    \n",
    "    runid = row[\"id\"]\n",
    "    run = api.run(f\"{WANDB_ENTITY_PROJECT}/{runid}\")\n",
    "    \n",
    "    if \"test/avg-precision\" in run.summary.keys():\n",
    "        print(f\"skipping {runid}\")\n",
    "        continue\n",
    "    \n",
    "    logdir = arow[\"logdir\"]\n",
    "    batch_size = int(arow[\"batch_size\"])\n",
    "    normal_class = arow[\"normal_class\"]\n",
    "    preproc = arow[\"preproc\"]\n",
    "    datadir = arow[\"datadir\"]\n",
    "        \n",
    "    net = FCDD(\n",
    "        in_shape=(224, 224),\n",
    "        model_name=\"FCDD_CNN224_VGG_F\",\n",
    "        # these values dont matter\n",
    "        optimizer_name=\"sgd\",  \n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        scheduler_name=\"lambda\",\n",
    "        scheduler_parameters=[.999],\n",
    "        loss_name=\"old-fcdd\",\n",
    "        dropout_mode=None,\n",
    "        dropout_parameters=[],\n",
    "    )\n",
    "    \n",
    "    snapshot_fpath = (dev_dir / logdir / \"snapshot.pt\").resolve()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        snapshot = torch.load(snapshot_fpath)\n",
    "\n",
    "    else:\n",
    "        snapshot = torch.load(snapshot_fpath, map_location=torch.device('cpu'))\n",
    "\n",
    "    net.load_state_dict(snapshot.pop('net', None))\n",
    "    net.eval();\n",
    "    \n",
    "    wandb_logger = WandbLogger(id=runid, project=WANDB_PROJECT, entity=WANDB_ENTITY,)\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        gpus=1, \n",
    "        logger=wandb_logger,  \n",
    "        callbacks=[\n",
    "            LogPrcurveCallback(\n",
    "                scores_key=\"score_maps\", gt_key=\"gtmaps\", log_curve=False, limit_points=None,\n",
    "                # doesnt matter because limit_points is None\n",
    "                python_generator=create_python_random_generator(0), stage=\"test\",\n",
    "            )\n",
    "        ], \n",
    "    )\n",
    "    \n",
    "    trainer.test(\n",
    "        model=net, \n",
    "        datamodule=MVTecAnomalyDetectionDataModule(\n",
    "            root=(dev_dir / datadir).resolve(),\n",
    "            normal_class=normal_class,\n",
    "            preprocessing=preproc,\n",
    "            preprocess_moment=DATAMODULE_PREPROCESS_MOMENT_BEFORE_BATCH_TRANSFER,\n",
    "            supervise_mode=SUPERVISE_MODE_REAL_ANOMALY,\n",
    "            batch_size=batch_size, \n",
    "            nworkers=0,\n",
    "            pin_memory=False,\n",
    "            seed=0,\n",
    "            raw_shape=(240, 240),\n",
    "            net_shape=(224, 224),\n",
    "            real_anomaly_limit=1,\n",
    "        )\n",
    "    ) \n",
    "    \n",
    "    wandb_logger.close()\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('fcdd_rc21')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "338d0fe2fa82b7a9b2c378f9bda9a6f667a440ddeb1b17bdd89b53b5abf6e642"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
